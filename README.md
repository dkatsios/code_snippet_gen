# CODE SNIPPET GENERATOR

## Docker
- There are two docker images available. One with remote (OpenAI) only functionality
and one with both remote and local. For the local one a good GPU is recommended.
  - To run only the remote model you can use `dkatsios/code_gen_remote_only`. 
  The image is much lighter.
  - To have both remote and local functionality use `dkatsios/code_gen_remote_local`. 
  It includes the Llamma-2 7b (chat Q5_K_M) model and can run on GPU.
- Run 
  ```
  docker run --rm \
      dkatsios/code_gen_remote_only \
      sh -c 'echo "OPENAI_API_KEY=<your-OpenAI-API-key>" > .env && \
      pytest test.py'
  ```
  to execute all the tests.
- Run `docker run -p 8000:8000 dkatsios/code_gen_remote_only` to start the server. 
Change the host image name / port if needed.


## UI

### Select Model
First, the user has to choose between the Local or Remote model.
For the Remote model, you will have to insert the OpenAI API key.

### Chat
Write a message and press Enter or Send.
- If the message is not related to coding, a related message will be displayed.
- If the message is related it will respond with
  - a text reply in case an explanation is needed
  - a code snippet otherwise

All the code snippets generated by the bot are stored and can be accessed by selecting `History`.

### History
Here are all the code snippets generated by the bot. By selecting one of them the user can:
- **Review**: The user can review the snippet. The content can be edited.
Any changes will be automatically saved and remain in History.
This will not affect the chat messages.
The code can also be executed by clicking on `Run`. The installed code interpreter (Python 3.11)
will be used. The result will be displayed below the code area.
- **Delete**: The snippet is removed from the History. It remains in the chat messages though.

## Cloud
The `remote_only` image has been deployed on <s>Azure</s> GCP  as a container app.
It can be accessed 
[here](https://code-gen-eqgdxjojvq-uc.a.run.app/)
(although the availability is not guarantied).


## Limitations
The Local model does not work as well as the Remote one. Sometimes the response includes
content that should not be returned. Further investigation is required.
Also the Local model takes some time to instantiate after you press `Proceed`. Be patient!